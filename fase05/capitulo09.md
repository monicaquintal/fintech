<div id="fase05" align="center">
<h1>FASE 5 - OOP</h1>
<h2>Capítulo 09: Inteligência Artificial.</h2>
</div>

<div align="center">
<h2>1. INTELIGÊNCIA ARTIFICIAL</h2>
</div>

- a grande maioria das empresas vem adotando essa tecnologia para automatizar processos e melhorar a experiência do consumidor final.

## 1.1 Mas o que é a Inteligência Artificial (ou IA)?

- refere-se ao desenvolvimento de sistemas de computadores ou máquinas que executam tarefas que, tipicamente, requerem inteligência humana. 
- envolve a criação de agentes inteligentes capazes de perceber seu ambiente, raciocinar sobre ele e tomar ações para atingir determinados objetivos.
- é a arte de criar máquinas que executam funções as quais requerem inteligência quando executadas por pessoas.
- visa simular e replicar habilidades cognitivas humanas, como aprendizado, resolução de problemas, reconhecimento de padrões, compreensão da linguagem e tomada de decisões.
- envolve o estudo de algoritmos, modelos estatísticos e técnicas computacionais para permitir que máquinas exibam comportamento inteligente.
- há `dois tipos principais de IA`:
<br>

a) `Inteligência Artificial Geral` (Artificial General Intelligence, ou General AI): 

  - Inteligência Artificial Forte. 
  - se refere a sistemas de Inteligência Artificial que exibem inteligência similar a humana e possui a habilidade de entender, aprender e aplicar conhecimento através de uma variedade de tarefas e domínios.
  - foco em replicar a versatilidade e adaptabilidade da inteligência humana. 
  - atualmente não há um exemplo de um sistema verdadeiramente de IA Geral.
<br>

b) `Inteligência Artificial Limitada` (Narrow Artificial Intelligence, ou Narrow AI):

  - Inteligência Artificial Fraca.
  - se refere a sistemas de Inteligência Artificial projetadas e desenvolvidas para executar tarefas ou funções específicas dentro de um domínio limitado.
  - são altamente especializados e se destacam em um número limitado de atividades.
  - se concentram na resolução de problemas bem definidos e não são capazes de generalizar seus conhecimentos para outros domínios ou tarefas. 
  - exemplos: assistentes de voz (Siri, Alexa, Google Assistant), sistemas de recomendação (em e-commerce, como na Amazon, em serviços de streaming, como Netflix, em redes sociais, como TikTok), reconhecimento de imagens (como reconhecimento facial, detecção de objetos).
  - depende de algoritmos, dados e regras predefinidas e específicas para executar suas tarefas com eficiência. Eles não possuem inteligência geral ou capacidade de entender o contexto além de seu domínio de conhecimento.

> O ***ChatGPT***, como outros modelos de linguagem (incluindo os Large-Language Model, ou LLM), é classificado como uma IA Limitada ou Fraca; ele pode gerar respostas contextualmente relevantes e coerentes, porém o serviço carece da compreensão, generalização e adaptabilidade mais amplas apresentadas pela inteligência humana ou pela definição pura da IA Geral. Ele opera com base em padrões e correlações estatísticas que aprendeu com grandes quantidades de dados de texto, em vez de possuir uma verdadeira compreensão ou consciência.

## 1.2 História da Inteligência Artificial

### 1.2.1 Tin Woodman, o personagem do livro O Mágico de Oz
- na primeira metade do século XX, a ficção científica trouxe ao mundo o conceito de robôs artificialmente inteligentes.

### 1.2.2 Alan Turing, o pai da ciência da computação
- na década de 1950, houve uma geração de cientistas, matemáticos e filósofos com o conceito de Inteligência Artificial culturalmente assimilado em suas mentes. 
- o mais conhecido deles é Alan Turing (pai da ciência da computação), matemático britânico que propôs a exploração das possibilidades matemáticas da Inteligência Artificial.
  - sugeriu que os humanos usam as informações disponíveis e a razão para resolver problemas e tomar decisões, então por que as máquinas não podem fazer o mesmo?
  - paper Computing Machinery and Intelligence (Máquinas Computacionais e Inteligência) escrito em 1950.
  - `Teste de Turing`:
    - inicia-se com base na pergunta: “As máquinas podem pensar?”.
    - são colocados dois humanos e um sistema de Inteligência Artificial no mesmo ambiente. 
    - um dos humanos é o interrogador, que fica separado por uma barreira do outro humano e da IA. 
    - o interrogador inicia uma conversa com o outro humano e com a máquina e, se ele não consegue distinguir se está falando com a IA ou com o outro humano, significa que o sistema passou no Teste de Turing.

### 1.2.3 Conferência de Dartmouth, surgimento do termo Inteligência Artificial
- o termo Inteligência Artificial foi utilizado pela primeira vez em 1955, na proposta de um projeto de pesquisa de verão de Dartmouth em Inteligência Artificial.
- reuniuram-se os principais pesquisadores de várias áreas para uma discussão sobre Inteligência Artificial, termo que eles escolheram utilizar para evitar destacar uma das linhas seguidas para o campo das “máquinas pensantes”.

> atualmente, a Inteligência Artificial é considerada “a teoria e o desenvolvimento de sistemas computacionais capazes de realizar tarefas que normalmente requerem inteligência humana, como percepção visual, reconhecimento de fala, tomada de decisão e tradução entre idiomas”.

### 1.2.4 ELIZA, o primeiro chatbot
- em 1965 foi criado o primeiro programa de computador, chamado ELIZA, para estudo de comunicação de linguagem natural entre homem e máquina.
- a ideia foi criar um script para simular uma conversa entre o paciente e o psicólogo (ELIZA).
- ELIZA foi projetada para imitar um terapeuta que faz perguntas abertas e até mesmo responde para continuar com o acompanhamento.
  - a máquina lê o texto e busca poruma palavra-chave. 
  - se a palavra é encontrada, a frase é gerada de acordo com uma regra associada à palavra-chave.
  - caso contrário, uma observação sem conteúdo ou, sob certas condições, uma frase gerada anteriormente é usada e, então, impressa na tela.
- ELIZA é considerada o primeiro chatbot da história da ciência da computação.

### 1.2.5 IBM Deep Blue
- em 1997, a IBM desenvolveu um computador (IBM® Deep Blue®), que derrotou o campeão mundial de xadrez da época. 
- O Deep Blue era capaz de explorar até 200 milhões de posições de xadrez possíveis por segundo.

### 1.2.6 IBM Watson
- em 2011, num programa americano de perguntas e respostas (Jeopardy!), o computador Watson derrotou os dois campeões de maior sucesso da história do programa.
- atualmente, o Watson oferece mais de dez serviços diferentes,disponíveis na plataforma de nuvem pública da IBM, a IBM Cloud; alguns deles são:
  - construção de interfaces conversacionais (base dos chatbots e assistentes virtuais).
  - reconhecimento de imagem.
  - tradutor entre idiomas.
  - processamento de Linguagem Natural (NLP).
  - classificador de texto.

### 1.2.7 Siri
- lançado como um aplicativo iOS em 2010. Dois meses depois, comprado pela Apple e integrado ao iPhone 4S em 2011.

### 1.2.8 Amazon Alexa
- assistente pessoal inteligente, lançada em 2014 com o alto-falante inteligente Amazon Echo. 
- utiliza Machine Learning e Inteligência Artificial para realizar atividades como tocar música, criar um evento no calendário, fazer buscas na internet,entremuitas outras ações.
- o que diferencia a Alexa da Siri e de outras assistentes, é a capacidade de permitir que desenvolvedores criem mais funções para a Alexa, conhecidascomo Alexa Skills.

### 1.2.9 AlphaGo
- é um programa de computador criado para jogar Go: um jogo de tabuleiro chinês considerado um dos mais antigos e que até hoje é jogado.
- a IA combina árvore de pesquisa avançada com redes neurais para analisar o tabuleiro de Go e selecionar o próximo movimento. 
- com o treino e tempo, o AlphaGo melhorou e se tornou cada vez mais eficiente no aprendizado e na tomada de decisões, processo conhecido como Reinforcement Learning (aprendizado por reforço).

### 1.2.10 ChatGPT e GPT-3, a revolução da IA Generativa
- em junho de 2020, a empresa OpenAI lançou uma versão limitada do GPT-3.
- o grande tamanho do modelo (com 175 bilhões de parâmetros) e sua capacidade de gerar textos coerentes e contextualmente relevante fizeram com que a tecnologia se destacasse na comunidade de inteligência artificial e atraísse o interesse do público.
- a OpenAI desenvolveu o ChatGPT, um modelo irmão do GPT-3, para melhorar especificamente as habilidades de conversação e criar uma experiência de chatbot mais envolvente e interativa.
- o modelo do ChatGPT utiliza um método de treinamento conhecido como Reinforcement Learning from Human Feedback(RLHR, ou Aprendizagem por Reforço com Feedback Humano). Assim, existem modelos de recompensa para o aprendizado por reforço responsáveis por recompensar o ChatGPT durante os treinamentos pela qualidade da resposta fornecida.
- o treinamento do ChatGPT foi finalizado no começo de 2022.

## 1.3 Por que IA?

- a IA deve substituir funções em empresas que podem ser automatizadas, como foi o caso da IBM que anunciou em 2023 que pausou a contratações de 7.800 vagas para automatizar com inteligência artificial. 
- por outro lado, compreender a IA permitirá que os profissionais aproveitem seu potencial para aprimorar seu trabalho, melhorar a eficiência e tomar decisões baseadas em dados. 

> `Digital Footprint`(ou vestígio tecnológico) são vestígios que deixamos na internet por meio de nossas ações; esses dados são ativos para grandes empresas, pois é a partir dos dados gerados, por exemplo, que é possível criar recomendações para os usuários.

- o primeiro passo para aprender sobre IA é entender os três tipos de dados:

### 1.3.1 Dados Estruturados

- referem-se ao dado que é organizado e armazenado em um formato predefinido com um schema ou modelo de dado bem definido. 
- segue uma estrutura consistente e é tipicamente armazenado em bancos de dados relacionais ou planilhas.
- características:
  - formato: 
    - dados são organizados em um formato fixo com uma estrutura clara e predefinida. 
    - geralmente representado no formato de tabelas com linhas e colunas, onde cada coluna corresponde a um atributo ou campo específico, e cada linha representa um registro de dados.
  - schema: 
    - os dados estruturados seguem um schema predefinido ou modelo de dados que define a estrutura, os tipos de dados e os relacionamentos entre diferentes elementos.
    - fornece uma forma de como os dados são organizados e permite consistência e integridade dos dados.
  - tipos de dados: 
    - cada atributo ou campo de um dado estruturado tem um tipo de dado definido, como um número, texto ou booleano. 
    - isso permite armazenamento, indexação e consulta eficientes dos dados.
  - relacionamentos: 
    - dados estruturados podem capturar relacionamentos entre diferentes entidades de dados por meio de chaves primárias, chaves estrangeiras ou outras associações definidas.
    - esses relacionamentos permitem unir tabelas e realizar consultas complexas em vários conjuntos de dados relacionados.
  - exemplos:
    - Bancos de dados relacionais: dados armazenados em tabelas com colunas e linhas fixas, gerenciados por sistemas de gerenciamento de banco de dados (DBMS) como MySQL, Oracle, ou Microsoft SQL Server.
    - Planilhas: dados organizados em linhas e colunas usando software como Microsoft Excel ou Google Sheets.
    - Arquivos CSV (Comma-Separated Values): arquivos de texto com dados tabular onde os valores estão separados por vírgulas.
    - Sistemas ERP (Enterprise Resource Planning): bancos de dados que armazenam dados estruturados relacionados aos processos do negócio como vendas, inventário e finanças.

> dados estruturados são adequados para consultas, relatórios e análises usando Structured Query Language (ou SQL) ou outras ferramentas para poder processar dado tabular de forma eficiente.

### 1.3.2 Dados Não Estruturados

- referem-se aos dados que não possuem um formato pré-definido ou estrutura organizado; não possuem um layout ou organização fixos.
- este tipo de dado não se encaixa perfeitamente em bancos de dados ou tabelas tradicionais e carece de um schema ou modelo consistente.
- dados não estruturados geralmente estão em sua forma bruta e não estão em conformidade com uma organização ou padrão específico.
- podem vir de várias formas, incluindo documentos de texto, e-mails, postagens de redes sociais, arquivos de áudio, imagens, vídeos, dados de sensores , etc.
- a coleta, o armazenamento e as análises realizadas com base nesses dados não são tarefas simples, mas, quando feitas corretamente, as informações que podem ser geradas a partir desse tipo de dado são valiosas.
- características:
  - falta de organização: 
    - não possuem uma estrutura ou formato adequado.
  - formatos variados:
    - podem existir em diversos formatos e podem não seguir um padrão consistente.
  - sem schema fixo: 
    - podem não aderir a um schema ou modelo de dados pré-definido. 
  - contexto e subjetividade:
    - geralmente possuem informações subjetivas, opiniões e nuances contextuais que os tornam mais difíceis de processar e analisar automaticamente.
- exemplos:
  - Dado textual:
    - dados de textos não estruturados podem incluir e-mails, páginas na internet, artigos, logs de chat, e documentos. 
    - este tipo de dado geralmente contém sentenças, parágrafos e texto em formato livre sem uma estrutura predefinida.
  - Dados multimídia:
    - incluem imagens, arquivos de áudio e vídeos.
    - esses arquivos carecem de uma estrutura específica e requerem técnicas especializadas, como visão computacional ou processamento de áudio, para extrair informações significativas.
  - dados de sensor: 
    - se referem a leituras brutas ou logs gerados por vários sensores ou dispositivos de Internet das Coisas (eInternet of Things ou IoT). 
    - podem consistir em leituras não formatadas, data/hora e outras informações contextuais.
  - dados de redes sociais:
    - como tweets, postagens, comentários, e conteúdos gerados. 
    - contêm textos não organizados, hashtags, menções de usuários e arquivos de mídia (foto, vídeo, gif).

> com os avanços em tecnologias como processamento de linguagem natural (Natural Language Processing ou NLP), visão computacional e algoritmos de Machine Learning, a extração de insights e significado dos dados não estruturados se tornam possíveis. Pré-processamento e transformação podem ser necessárias para converter dados não estruturados em um formato estruturado ou semiestruturado.

### 1.3.3 Dados Semiestruturados

- tipo de dado que não está em conformidade com a estrutura estrita dos bancos de dados relacionais tradicionais, mas possui algum nível de organização e flexibilidade. 
- contém elementos estruturados e não estruturados, permitindo certo grau de variabilidade em seu formato e organização.
- não seguem uma estrutura rígida, mas geralmente contém tags, labels, ou marcadores que fornecem um nível de organização ou contexto.
- são normalmente representados em formatos como XML (eXtensible Markup Language), JSON (JavaScript Object Notation) ou HTML (Hypertext Markup Language), formatos que permitem a inclusão de metadados, estruturas hierárquicas ou pares de chave-valor, fornecendo algum nível de estrutura e organização.
- características:
  - flexibilidade: 
    - exibem flexibilidade em seu formato e estrutura. 
    - permitem variações na organização e representação dos elementos de dados.
    - a ausência de schemas rígidos ou estruturas predefinidas permite a adição, remoção ou modificação de campos de dados conforme necessário.
  - estrutura parcial: 
    - contêm algum nível de estrutura ou organização; geralmente inclui metadados, labels ou tags que fornecem contexto ou descrevem os elementos de dados, facilitando a interpretação e o processamento.
  - representação hierárquica ou aninhada (Nested):
    - podem ter uma estrutura hierárquica ou aninhada, o que permite o agrupamento de elementos de dados relacionados, criando relacionamentos pai-filho ou representando relacionamentos complexos entre entidades.
    - permite capturar semânticas e relacionamentos mais ricos nos dados.
  - schema variável:
    - não possuem um schema fixo como os dados estruturados: permitem schemas dinâmicos em que diferentes instâncias dos dados podem ter vários conjuntos de campos ou atributos.
  - dados textuais e não textuais: 
    - pode incluir texto de formato livre, elementos multimídia ou uma combinação de ambos, permitindo integração de diversos tipos de informações dentro de uma única entidade de dados semiestruturados.
  - representações comuns:
    - comumente representados em formatos como XML (eXtensible Markup Language), JSON (JavaScript Object Notation) ou HTML (Hypertext Markup Language), formatos que fornecem flexibilidade e organização.
- exemplos:
  - Documentos XML:
    - representação do dado em um formato estruturado usando tags para definir elementos e atributos para fornecer informação adicional. 
    - fornece uma forma flexível e extensiva para armazenar e trocar dados, comumente usado em serviços da Web e intercâmbio de documentos.
  - Arquivos JSON:
    - é um formato de intercâmbio de dados leve que usa uma estrutura de par chave-valor.
    - amplamente utilizado para representar dados em APIs da Web, arquivos de configuração e bancos de dados NoSQL. 
    - fornece flexibilidade na definição de elementos de dados e permite estruturas aninhadas.
  - Arquivos de log:
    - contêm entradas com registro de data e hora, descrições de eventos e campos de dados variáveis.
    - comumente usados para solução de problemas, análise de desempenho e propósitos de auditoria.

> Tecnologias como analisadores de XML, analisadores de JSON ou algoritmos de análise personalizados podem ser usados para extrair informações relevantes de dados semiestruturados e convertê-los em uma apresentação mais estruturada para análise e integração com outros sistemas.

### 1.3.4 Expectativa do crescimento dos dados no mundo

- um estudo da Seagate UK mostra que, até 2025, haverá 175 zettabytes de dados gerados por mais de 6 bilhões de pessoas, sendo grande parte das interações com dados graças aos dispositivos IoT (Internet of Thing), responsáveis por gerar mais de 90 ZB de dados.
  - 1 zettabyte (1 ZB) = 1.000ˆ7 ou 1.000.000.000.000.000.000.000 bytes.
- e, segundo o IDC, 80% dos dados no mundo serão não estruturados até 2025 (140 ZB). 
- hoje o grande desafio das empresas é trazer mais valor com essa grande quantidade de dados.
  - como a maior parte dos dados gerados são não estruturados, as empresas precisam investir em tecnologia para poder analisar cada tipo de dado.

<div align="center">
<h2>2. INTELIGÊNCIA ARTIFICIAL, MACHINE LEARNING E DEEP LEARNING</h2>
</div>

- dentro do campo de estudo da Inteligência Artificial, há subcampos: 
  - Machine Learning (Aprendizado de Máquina) é um subcampo da Inteligência Artificial.
    - Deep Learning (Aprendizagem Profunda) é um subcampo do Machine Learning. 
      - E Neural Network (Redes Neurais) é um subcampo de Deep Learning.

1. `Inteligência Artificial` (IA): 
- campo mais amplo de criação de máquinas inteligentes que podem simular a inteligência humana e executar tarefas que normalmente requerem inteligência humana.
- envolve desenvolvimento de algoritmos e sistemas que podem perceber, raciocinar, aprender e tomar decisões.
- abrange várias técnicas, metodologias e abordagens para imitar a inteligência humana em máquinas.

2. `Machine Learning` (ML):
- subconjunto da IA.
- se concentra no desenvolvimento de algoritmos e modelos que permitem que os computadores aprendam com os dados e façam previsões ou executem ações sem serem explicitamente programados. 
- envolve o treinamento de um modelo em um determinado conjunto de dados, permitindo identificar padrões, relacionamentos e insights.
- os algoritmos de ML aprendem iterativamente com os dados, melhorando seu desempenho com a experiência.

3. `Deep Learning` (DL):
- subcampo do Machine Learning.
- se concentra especificamente em redes neurais artificiais inspiradas na estrutura e no funcionamento do cérebro humano.
- seus modelos, conhecidos como redes neurais profundas (Deep Neural Networks), consistem em várias camadas de neurônios interconectados que aprendem representações hierárquicas de dados.
- essas redes podem aprender automaticamente padrões e recursos complexos diretamente de dados brutos, sem a necessidade de engenharia manual de recursos.

## 2.1 Machine Learning (ou ML)

- é uma ramificação da Inteligência Artificial, com o foco na construção de aplicações que aprendem com os dados e cuja acuracidade melhora à medida do tempo.
- os algoritmos são treinados para encontrar padrões e funcionalidades em uma grande massa de dados para possibilitar a tomada de melhores decisões e até a criação de previsões com base em dados.
- assistentes virtuais (Lu do Magazine Luiza, Alexa da Amazon) e plataformas de recomendação de produtos, filmes e músicas (como YouTube, Netflix e Spotify) são exemplos do uso de ML para melhorar a experiência do produto, além de buscarem melhorar a acuracidade de suas falas e recomendações à medida que vão analisando mais interações.
- há `três categorias principais de ML`:

1. `Supervised Machine Learning` (Aprendizado de Máquina Supervisionado):
- utiliza um modelo de treinamento em que você fornece os dados e os agrupa em rótulos.
- exemplo prático de quando você precisa treinar as funções de reconhecimento de um ***chatbot***: você define as frases que serão usadas para que o robô identifique, por exemplo, uma saudação ou um agradecimento; os dados estarão agrupados em um rótulo (por exemplo, saudação).

2. `Unsupervised Machine Learning` (Aprendizado de Máquina Não Supervisionado):
- utiliza um modelo de treinamento em que você fornece uma grande quantidade de dados e os algoritmos os agruparão de acordo com algumas similaridades.
- esse método visa a identificação de padrões e relacionamentos em dados que provavelmente poderiam ser perdidos, se usado o método de Aprendizado de Máquina Supervisionado, por conta da intervenção humana. 
- um exemplo de uso são os filtros de spam: há mais e-mails do que um cientista de dados poderia analisar, então ao invés de ter um time que analisa cada e-mail, um algoritmo de aprendizagem não supervisionado pode analisar grandes volumes de e-mails e, por meio dos recursos e padrões que indicam spam, classificaria se o e-mail recebido é ou não um spam.

3. `Semi-supervised Machine Learning` (Aprendizagem Semissupervisionada):
- oferece um meio-termo entre os dois modelos acima. 
- esse método permite que você utilize um pequeno conjunto de dados rotulados para orientar a classificação e o agrupamento de um conjunto de dados maior e não rotulado.
- resolve o problema de não haver dados rotulados suficientes (ou por não ter como agrupar dados suficientes) para treinar um algoritmo de aprendizagem supervisionada. 
- um modelo de classificação de documentos de texto é um exemplo de uso de um modelo de Aprendizagem Semissupervisionada, isso porque é muito trabalhoso e, também, pouco eficiente para uma pessoa realizar esse trabalho. Então, você pode inserir alguns documentos, fazer as marcações (definir os rótulos) e, depois, aplicar o algoritmo para classificar uma grande quantidade de documentos de texto, sem nenhum rótulo.

> além dos três métodos acima, há também o `Reinforcement Machine Learning` (Aprendizado de Máquina por Reforço), parecido com o Supervised Machine Learning, porém o algoritmo não é treinado com dados de exemplo: o modelo aprende por meio de tentativa e erro. No programa de TV Jeopardy!, o sistema do computador Watson utilizou esse método para definir, durante a competição, se tentava dar uma resposta ou não.

## 2.2 Deep Learning (Aprendizagem Profunda ou DL)

- é um subcampo do Machine Learning que foca na construção e treinamento de redes neurais profundas (Deep Neural Networks), que são redes neurais artificiais com várias camadas.
- inspirado na estrutura e funcionamento do cérebro humano, o DL visa modelar padrões complexos e relacionamentos em dados usando uma representação hierárquica de recursos.
- "deep": refere-se à profundidade da rede neural - ela possui várias camadas ocultas entre as camadas de entrada e saída. 
  - cada camada consiste em nós interconectados (neurônios artificiais) que executam cálculos nos dados de entrada. 
  - ao empilhar várias camadas, os modelos de DL podem aprender representações cada vez mais abstratas e sofisticadas dos dados.
- modelos de DL ***normalmente são não supervisionados ou semisupervisionados***, e ***modelos de aprendizado por reforço também podem ser modelos de DL***.
- `principais características`:

1. Neural Networks (Redes Neurais):
- Convolutional Neural Networks (Redes Neurais Convolucionais, ou CNNs) para processamento de imagens,
- Recurrent Neural Networks (Redes Neurais Recorrentes, ou RNNs) para dados de sequência e 
- Generative Adversarial Networks (Redes Adversárias Generativas, ou GANs) para tarefas generativas.

2. Feature Learning (Aprendizado de Recursos): 
- modelos de DL podem aprender automaticamente representações ou recursos significativos de dados brutos, eliminando a necessidade de engenharia manual de recursos.
- as camadas ocultas da rede extraem recursos hierárquicos dos dados de entrada, permitindo que o modelo capture padrões complexos e relacionamentos complexos.

3. Big Data e Parallel Computing (Computação Paralela):
- algoritmos de DL prosperam em conjuntos de dados de grande escala, pois exigem quantidades substanciais de dados rotulados para treinar modelos complexos.
- cálculos de Deep Learning são computacionalmente intensivos e se beneficiam da computação paralela em GPUs (Graphics Processing Units)   ou aceleradores de hardware especializados.

4. Deep Neural Network Training (Treinamento de Rede Neural Profunda):
- envolve um processo chamado retro propagação, em que o modelo ajusta seus parâmetros internos (pesos e desvios) para minimizar a diferença entre as saídas previstas e as saídas reais. 
- técnicas de otimização como descida de gradiente estocástico (SGD) e variações como Adam e RMSprop são comumente usadas para treinamentos eficientes.

> O Deep Learning demonstrou desempenho excepcional em várias tarefas, incluindo reconhecimento de imagem e fala, processamento de linguagem natural, tradução automática, sistemas de recomendação e muito mais.

## 2.3 Computação cognitiva

- pode ser considerada um campo interdisciplinar que engloba elementos de IA, Machine Learning e Deep Learning.
  - a IA concentra-se amplamente na criação de máquinas inteligentes.
  - ML e DL empregadas para processar e analisar dados, extrair insights significativos e tomar decisões informadas.
- ***objetivo:*** simular processos de pensamento humano e habilidades cognitivas em máquinas, permitindo que percebam, raciocinem, aprendam e tomem decisões de maneira mais humana.
- visa ir além da programação tradicional baseada em regras e aproveitar técnicas como processamento de linguagem natural, visão computacional e reconhecimento de padrões para entender e interpretar dados de maneira mais sutil e sensível ao contexto.
- sistemas de Computação Cognitiva são projetados para aprender e se adaptar continuamente com base no feedback e em novas informações melhorando seu desempenho ao longo do tempo. 

> A Computação Cognitiva integra IA, Machine Learning e técnicas de Deep Learning para criar sistemas que podem imitar os processos do pensamento humano e interagir com humanos de maneira mais inteligente e natural. O objetivo é preencher a lacuna entre cognição humana e a inteligência da máquina, permitindo que as máquinas entendam, aprendam e tomem decisões em ambientes complexos e não estruturados.

## 2.4 IA Generativa

- tipo de inteligência artificial que pode criar ou gerar novos conteúdos, como imagens, textos, músicas ou até mesmo vídeos. É como ensinar um computador a ser criativo e produzir material original.
- em vez de serem explicitamente programados com regras específicas, os modelos de IA Generativa são treinados em grandes quantidades de dados e aprendem padrões e relacionamentos dentro desses dados.
- conceitos fundamentais:

1. Modelos generativos: 
- são algoritmos ou modelos que aprendem a gerar novas amostras de dados semelhantes aos dados de treinamento aos quais foram expostos.
- capturam a distribuição subjacente dos dados de treinamento e podem gerar novas amostras que exibem características semelhantes.

2. Dados de treinamento:
- permitem que os modelos generativos aprendam e capturem os padrões e estruturas presentes nos dados.
- a qualidade e a diversidade dos dados de treinamento desempenham um papel crucial no desempenho e na criatividade do modelo generativo.

3. Redes Neurais:
- muitos modelos generativos, como Variational Autoencoders (VAEs) e Generative Adversarial Networks(GANs), utilizam redes neurais como sua arquitetura subjacente.
- as redes neurais são compostas por nós ou neurônios interconectados que realizam cálculos, permitindo que o modelo aprenda representações complexas e gere novos dados.

4. Espaço Latente:
- consiste em uma representação comprimida dos dados de treinamento.
- ele captura a estrutura subjacente dos dados e permite que o modelo gere novas amostras por amostragem desse espaço.
- é uma representação de dimensão inferior dos dados originais.

5. Variational Autoencoders (VAEs):
- são modelos generativos que combinam os princípios de autoencoders e modelagem probabilística.
- visam aprender uma representação comprimida dos dados de entrada e gerar novas amostras por amostragem de um espaço latente aprendido.
- frequentemente usados para tarefas como geração de imagens e síntese de dados.

6. Generative Adversarial Networks (GANs): 
- consistem em duas redes neurais: um gerador e um discriminador.
  - a rede geradora gera novas amostras,
  - a rede discriminadora tenta distinguir entre amostras reais e geradas.
- as duas redes são treinadas juntas em um cenário competitivo, onde o gerador visa enganar o discriminador, e o discriminador visa tornar-se melhor em distinguir amostras reais de amostras geradas.
- amplamente utilizadas para tarefas como geração de imagens e vídeos.

7. Reinforcement Learning:
- aprendizado por reforço pode ser combinado com modelos generativos para criar sistemas de IA que podem aprender a gerar conteúdo com base em sinais de recompensa. 
- ao formular o processo de geração como um problema de tomada de decisão sequencial, o aprendizado por reforço pode guiar o modelo generativo para produzir os resultados desejados, fornecendo feedback na forma de recompensas ou penalidades.

> Em resumo, a IA generativa é um tipo de IA que pode criar novos conteúdos com base nos padrões que aprendeu com os dados existentes. É como ensinar um computador a ser criativo e criar material original, sejam imagens, texto, música ou mais.

## 2.5 Mercado de trabalho

- há um site chamado [“Will Robots Take my Job?”](https://willrobotstakemyjob.com/), que permite realizar busca por uma profissão e ele aponta a probabilidade de a profissão ser substituída por um robô.

---

## FAST TEST

### 1. Qual o tipo de IA que se refere a sistemas de Inteligência Artificial que exibem inteligência similar a humana e possui a habilidade de entender, aprender e aplicar conhecimento através de uma variedade de tarefas e domínios?
> Inteligência Artificial Geral.

### 2. Em junho de 2020, a empresa OpenAI lançou uma versão limitada do GPT-3. Quais os recursos que a OpenAI apresentou do GPT-3?
> Conclusão de texto, tradução, resposta a perguntas e até redação criativa.

### 3. Sobre IA Generativa, selecione a afirmativa correta:
> Refere-se a um tipo de inteligência artificial que pode criar ou gerar novos conteúdos, como imagens, textos, músicas ou até mesmo vídeos. É como ensinar um computador a ser criativo e produzir material original.

--- 

[Voltar ao início!](https://github.com/monicaquintal/fintech)